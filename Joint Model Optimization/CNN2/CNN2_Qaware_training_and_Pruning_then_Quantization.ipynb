{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DTyaborA9Uvt"
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aa7Yuka-9CE2"
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y2MkpT2q_fjG"
   },
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wgplOk-H9UQA"
   },
   "source": [
    "### Loading MNIST DIGITS Dataset and Training CNN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "YApCbSvg9aCF",
    "outputId": "73d357ec-0b47-4af7-987e-a793020c455d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 14s 230us/sample - loss: 0.2848 - accuracy: 0.9200 - val_loss: 0.1382 - val_accuracy: 0.9596\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 16s 259us/sample - loss: 0.1130 - accuracy: 0.9675 - val_loss: 0.0818 - val_accuracy: 0.9756\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 13s 209us/sample - loss: 0.0806 - accuracy: 0.9765 - val_loss: 0.0755 - val_accuracy: 0.9755\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 13s 220us/sample - loss: 0.0659 - accuracy: 0.9807 - val_loss: 0.0645 - val_accuracy: 0.9778\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 13s 214us/sample - loss: 0.0569 - accuracy: 0.9830 - val_loss: 0.0597 - val_accuracy: 0.9787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8435957780>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images.astype(np.float32) / 255.0\n",
    "test_images = test_images.astype(np.float32) / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=5,\n",
    "  validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAzOk7uW91HL"
   },
   "outputs": [],
   "source": [
    "# Saving Model\n",
    "model.save('1_digits_mnist_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_m2WJ8kBJF4"
   },
   "source": [
    "### CNN2: Original model's (1_digits_mnist_model.h5) accuracy, model load and inference time and unit Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bI_GR8XYAznF"
   },
   "outputs": [],
   "source": [
    "# Load trained .h5 model\n",
    "model = tf.keras.models.load_model('./1_digits_mnist_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "ak3QjOXk_Rkq",
    "outputId": "b912502c-bf0a-4168-c2ac-4201b21a0ddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 0.9787\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test set\n",
    "score = model.evaluate(test_images, test_labels, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OL_HUPslBUYN"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "data = test_images[0]\n",
    "data = data.reshape((1, 28, 28))\n",
    "def orig_model_infer_time():\n",
    "  start_time_full = time.time()\n",
    "  model = tf.keras.models.load_model('./1_digits_mnist_model.h5', custom_objects=None, compile=True)\n",
    "  start_time_infer = time.time()\n",
    "  model.predict(data)\n",
    "  results = {'Time to load model and then infer': (time.time() - start_time_full),\n",
    "             'Time to only infer': (time.time() - start_time_infer)}\n",
    "  \n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "k1yKnOBuBZVp",
    "outputId": "24948761-8bd0-4a89-a4a4-9a01e794838c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Time to load model and then infer': 0.2886533737182617,\n",
       " 'Time to only infer': 0.05460047721862793}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_model_infer_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_m2WJ8kBJF4"
   },
   "source": [
    "### Quantization-aware (Q-aware) training of CNN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "id": "twgJvGD5IGpv",
    "outputId": "d5225d90-f348-40c1-81af-e173c8958043",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_model_optimization in /home/db/.virtualenvs/LR/lib/python3.6/site-packages (0.3.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /home/db/.virtualenvs/LR/lib/python3.6/site-packages (from tensorflow_model_optimization) (0.1.5)\n",
      "Requirement already satisfied: numpy~=1.14 in /home/db/.virtualenvs/LR/lib/python3.6/site-packages (from tensorflow_model_optimization) (1.17.3)\n",
      "Requirement already satisfied: six~=1.10 in /home/db/.virtualenvs/LR/lib/python3.6/site-packages (from tensorflow_model_optimization) (1.12.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/db/.virtualenvs/LR/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f83d022cac8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f83d022cac8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7f83d020c9e8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitConvQuantizeConfig object at 0x7f83d020c9e8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f83d01bfeb8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f83d01bfeb8>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f83d01ca128>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f83d01ca128>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f83d01ca358>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "WARNING: AutoGraph could not transform <bound method Default8BitQuantizeConfig.set_quantize_activations of <tensorflow_model_optimization.python.core.quantization.keras.default_8bit.default_8bit_quantize_registry.Default8BitQuantizeConfig object at 0x7f83d01ca358>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: expected an indented block (<unknown>, line 14)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quant_reshape (QuantizeWrapp (None, 28, 28, 1)         1         \n",
      "_________________________________________________________________\n",
      "quant_conv2d (QuantizeWrappe (None, 26, 26, 12)        147       \n",
      "_________________________________________________________________\n",
      "quant_max_pooling2d (Quantiz (None, 13, 13, 12)        1         \n",
      "_________________________________________________________________\n",
      "quant_flatten (QuantizeWrapp (None, 2028)              1         \n",
      "_________________________________________________________________\n",
      "quant_dense (QuantizeWrapper (None, 10)                20295     \n",
      "=================================================================\n",
      "Total params: 20,445\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 35\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_model_optimization\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for for quantization aware.\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZGSHxVNVIPq4",
    "outputId": "30f5d447-dda8-4801-d7de-2bf27b464030"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/5\n",
      "54000/54000 [==============================] - 27s 502us/sample - loss: 0.0602 - accuracy: 0.9817 - val_loss: 0.0447 - val_accuracy: 0.9865\n",
      "Epoch 2/5\n",
      "54000/54000 [==============================] - 29s 535us/sample - loss: 0.0518 - accuracy: 0.9836 - val_loss: 0.0522 - val_accuracy: 0.9853\n",
      "Epoch 3/5\n",
      "54000/54000 [==============================] - 26s 474us/sample - loss: 0.0456 - accuracy: 0.9861 - val_loss: 0.0484 - val_accuracy: 0.9863\n",
      "Epoch 4/5\n",
      "54000/54000 [==============================] - 27s 492us/sample - loss: 0.0399 - accuracy: 0.9873 - val_loss: 0.0505 - val_accuracy: 0.9843\n",
      "Epoch 5/5\n",
      "54000/54000 [==============================] - 26s 485us/sample - loss: 0.0353 - accuracy: 0.9887 - val_loss: 0.0500 - val_accuracy: 0.9863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f840c59b860>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and evaluate the model against baseline\n",
    "\n",
    "# train_images_subset = train_images[0:1000] # out of 60000\n",
    "# train_labels_subset = train_labels[0:1000]\n",
    "\n",
    "q_aware_model.fit(train_images, train_labels,\n",
    "                  batch_size=10, epochs=5, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "MEorEW-sIT9B",
    "outputId": "005b0964-854b-4d19-fdeb-9a70499fb3ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 0.981\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test set\n",
    "score = q_aware_model.evaluate(test_images, test_labels, verbose=0)\n",
    "\n",
    "# Print test accuracy\n",
    "\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2UeR9QhCJATK"
   },
   "outputs": [],
   "source": [
    "q_aware_model.save('2_digits_mnist_model_qaware.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_m2WJ8kBJF4"
   },
   "source": [
    "### Q-aware trained models unit inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "data = test_images[0]\n",
    "data = data.reshape((1, 28, 28))\n",
    "print (data.shape)\n",
    "data_y = train_labels[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Time to only infer': 0.03508925437927246}\n"
     ]
    }
   ],
   "source": [
    "# Unit inference time \n",
    "start_time_infer = time.time()\n",
    "score = q_aware_model.evaluate(data, data_y, verbose=0)\n",
    "results1 = {'Time to only infer': (time.time() - start_time_infer) }\n",
    "print (results1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D8Iv3s1tEphP"
   },
   "source": [
    "### Pruning CNN2 (3_digits_mnist_model_pruning.h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "jFzu772QD5Va",
    "outputId": "12074f20-931d-454a-ed3d-fb233ab25eab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\r\n",
      "You should consider upgrading via the '/home/db/.virtualenvs/LR/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "QlYk1SSmCXlN",
    "outputId": "967c543d-0156-4a9c-8873-f26d926839f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/db/.virtualenvs/LR/lib/python3.6/site-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:219: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_reshape  (None, 28, 28, 1)         1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_conv2d ( (None, 26, 26, 12)        230       \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_max_pool (None, 13, 13, 12)        1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_flatten  (None, 2028)              1         \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense (P (None, 10)                40572     \n",
      "=================================================================\n",
      "Total params: 40,805\n",
      "Trainable params: 20,410\n",
      "Non-trainable params: 20,395\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 128\n",
    "epochs = 2\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "num_images = train_images.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "duNC1YlWD1vL",
    "outputId": "aaf46d80-cc6c-47a4-b747-5dbbf8143e89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/2\n",
      "54000/54000 [==============================] - 11s 200us/sample - loss: 0.0751 - accuracy: 0.9793 - val_loss: 0.0799 - val_accuracy: 0.9813\n",
      "Epoch 2/2\n",
      "54000/54000 [==============================] - 9s 174us/sample - loss: 0.0930 - accuracy: 0.9733 - val_loss: 0.0696 - val_accuracy: 0.9812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f84a18ba978>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "  \n",
    "model_for_pruning.fit(train_images, train_labels,\n",
    "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qkTLtX5YELvI",
    "outputId": "54374e13-8c61-4e82-b2ff-23b25eebed82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned test accuracy: 0.9728\n"
     ]
    }
   ],
   "source": [
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
    "   test_images, test_labels, verbose=0)\n",
    "\n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned Keras model to: ./3_digits_mnist_model_pruning.h5\n"
     ]
    }
   ],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "pruned_keras_file = './3_digits_mnist_model_pruning.h5'\n",
    "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', pruned_keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "{'Time to load model and then infer': 0.13429832458496094}\n",
      "{'Time to only infer': 0.07329654693603516}\n"
     ]
    }
   ],
   "source": [
    "# This cell is usesd to find the Time to load model and then infer and Time to only infer\n",
    "import time\n",
    "data = test_images[0]\n",
    "data = data.reshape((1, 28, 28))\n",
    "def orig_model_infer_time():\n",
    "  start_time_full = time.time()\n",
    "  model = tf.keras.models.load_model('./3_digits_mnist_model_pruning.h5', custom_objects=None, compile=True)\n",
    "  start_time_infer = time.time()\n",
    "  model.predict(data)\n",
    "  results = {'Time to load model and then infer': (time.time() - start_time_full)}\n",
    "  results1 = {'Time to only infer': (time.time() - start_time_infer) }\n",
    "  print (results)\n",
    "  print (results1)\n",
    "    \n",
    "orig_model_infer_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMAjskGHESh2"
   },
   "outputs": [],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "tf.keras.models.save_model(model_for_export, '3_digits_mnist_model_pruning.h5', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y048hlBm_UtZ"
   },
   "source": [
    "### Convert CNN2 (1_digits_mnist_model.h5) to TF Lite (4_digits_mnist_model_tflite.tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8AoD736A_ZRK"
   },
   "outputs": [],
   "source": [
    "# Load trained .h5 model\n",
    "model = tf.keras.models.load_model('./1_digits_mnist_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5x0iUxA_WxD"
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model_file = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "M4namwcX_sVV",
    "outputId": "4cd4863b-d7ae-4c9c-b3ef-c59a0a44fa61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83280"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_models_dir = pathlib.Path(\"digits_mnist_tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "tflite_model_file_dir = tflite_models_dir/\"4_digits_mnist_model_tflite.tflite\"\n",
    "tflite_model_file_dir.write_bytes(tflite_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e5iW0Z_s_zzt"
   },
   "source": [
    "### Convert CNN2 (1_digits_mnist_model.h5) to Integer with float fallback Quantized version (5_digits_mnist_Integer_float_model.tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nglrk2bW_2Uj"
   },
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fePb_H5CAC1I"
   },
   "outputs": [],
   "source": [
    "mnist_train, _ = tf.keras.datasets.mnist.load_data()\n",
    "images = tf.cast(mnist_train[0], tf.float32) / 255.0\n",
    "mnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\n",
    "def representative_data_gen():\n",
    "  for input_value in mnist_ds.take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L0tyxKs1AD1A",
    "outputId": "f8cc5680-f726-4337-ac5d-91587c9fb0b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23208"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_quant = converter.convert()\n",
    "tflite_model_quant_file = tflite_models_dir/\"5_digits_mnist_Integer_float_model.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PPNTSd2yASgC"
   },
   "source": [
    "### Convert CNN2 (1_digits_mnist_model.h5) to Float Quantized version (6_digits_mnist_float16_model.tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YRzA6S8vAM2k"
   },
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "KREhYrVxAYzU",
    "outputId": "6a038159-665c-4871-c7d6-ba4d706a2b86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42972"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_fp16_model = converter.convert()\n",
    "tflite_model_fp16_file = tflite_models_dir/\"6_digits_mnist_float16_model.tflite\"\n",
    "tflite_model_fp16_file.write_bytes(tflite_fp16_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CNKzdQdJAn9A"
   },
   "source": [
    "### Convert CNN2 (1_digits_mnist_model.h5) to Integer Only Quantized version (7_digits_mnist_Integer_model.tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ASnV56vOA3vc"
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gs6aSYpwAj8C"
   },
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CE35vdSIAtbB"
   },
   "outputs": [],
   "source": [
    "mnist_train, _ = tf.keras.datasets.mnist.load_data()\n",
    "images = tf.cast(mnist_train[0], tf.float32) / 255.0\n",
    "mnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\n",
    "def representative_data_gen():\n",
    "  for input_value in mnist_ds.take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8EpZiTtSAuq0",
    "outputId": "c5da203c-63c7-4e0a-a6cc-515a11f9bf62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23208"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "tflite_int_quant_model = converter.convert()\n",
    "tflite_model_integeronly_file = tflite_models_dir/\"7_digits_mnist_Integer_model.tflite\"\n",
    "tflite_model_integeronly_file.write_bytes(tflite_int_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_7Vs4vCCNlN"
   },
   "source": [
    "### Evaluate Post training Quantized versions of CNN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "zGsikGp9CQW8",
    "outputId": "81d56095-01bc-4fa1-90fd-500687d00674"
   },
   "outputs": [],
   "source": [
    "# Evaluate the mode\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "def evaluate_model(interpreter):\n",
    "  start_time = time.time()\n",
    "\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for test_image in test_images:\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "  \n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  accurate_count = 0\n",
    "  for index in range(len(prediction_digits)):\n",
    "    if prediction_digits[index] == test_labels[index]:\n",
    "      accurate_count += 1\n",
    "  accuracy = accurate_count * 1.0 / len(prediction_digits)\n",
    "\n",
    "  results = {'time': (time.time() - start_time),\n",
    "             'accuracy': accuracy}\n",
    "\n",
    " \n",
    "  # Loading Test Image\n",
    "  test_img = np.expand_dims(test_images[0], axis=0).astype(np.float32)\n",
    "\n",
    "  interpreter.set_tensor(input_index, test_img)\n",
    "  start_time_infer = 0\n",
    "  start_time_infer = time.time()\n",
    "  interpreter.invoke()\n",
    "\n",
    "  predictions = interpreter.get_tensor(output_index)\n",
    "\n",
    "  result1 = {\"Time to only Infer\" : (time.time() - start_time_infer),\n",
    "            \"Time to load Quantized model and Infer \": (time.time() - start_time)}\n",
    "  \n",
    "  return results, result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CadvCseDBsr8"
   },
   "source": [
    "### CNN2: Integer with float fallback quantized model's accuracy and inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y3lKhc03EHWp"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "tflite_model_file = tflite_models_dir/'5_digits_mnist_Integer_float_model.tflite'\n",
    "interpreter = tf.lite.Interpreter(model_path=str(tflite_model_file))\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1_gJ1gC4B71p",
    "outputId": "5714f73a-e5ae-4a07-d215-8a9968c89794"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'time': 1.6143968105316162, 'accuracy': 0.9539},\n",
       " {'Time to only Infer': 0.00014972686767578125,\n",
       "  'Time to load Quantized model and Infer ': 1.6145589351654053})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(interpreter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e4zE--ZSFGXY"
   },
   "source": [
    "### CNN2: Float quantized model's accuracy and inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m5U3ptQRDzsB"
   },
   "outputs": [],
   "source": [
    "tflite_float_model_file = tflite_models_dir/'6_digits_mnist_float16_model.tflite'\n",
    "interpreter_float = tf.lite.Interpreter(model_path=str(tflite_float_model_file))\n",
    "interpreter_float.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WpERUsZpFPd9",
    "outputId": "279908d1-dabf-4917-929c-9be5aa33de67"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'time': 1.0327961444854736, 'accuracy': 0.7017},\n",
       " {'Time to only Infer': 0.00010633468627929688,\n",
       "  'Time to load Quantized model and Infer ': 1.0329153537750244})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(interpreter_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EaT2lW_2FVwa"
   },
   "source": [
    "### CNN2: Integer Only Quantized Model's Accuracy and Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OrLvpHBwFT9t"
   },
   "outputs": [],
   "source": [
    "tflite_int_model_file = tflite_models_dir/'7_digits_mnist_Integer_model.tflite'\n",
    "interpreter_int = tf.lite.Interpreter(model_path=str(tflite_int_model_file))\n",
    "interpreter_int.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "U44oub_UFf6X",
    "outputId": "70590779-9754-42de-eb4f-e247f2fe19a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'time': 1.638054370880127, 'accuracy': 0.9539},\n",
       " {'Time to only Infer': 0.0001494884490966797,\n",
       "  'Time to load Quantized model and Infer ': 1.6382172107696533})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(interpreter_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EaT2lW_2FVwa"
   },
   "source": [
    "## Joint model optimization of CNN2 - applying both pre + post training optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D8Iv3s1tEphP"
   },
   "source": [
    "### Convert pruned CNN2 (3_digits_mnist_model_pruning.h5) into TFlite (8_digits_mnist_model_pruning.tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMAjskGHESh2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "  model = tf.keras.models.load_model('./3_digits_mnist_model_pruning.h5', custom_objects=None, compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GanX5DneFHqK",
    "outputId": "a5f97e82-646e-4fe5-f5f5-d2e6d9d1c9e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned TFLite model to: 8_digits_mnist_model_pruning.tflite\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "pruned_tflite_file = tflite_models_dir/'8_digits_mnist_model_pruning.tflite'\n",
    "\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "  f.write(pruned_tflite_model)\n",
    "\n",
    "print('Saved pruned TFLite model to:', pruned_tflite_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HeSJbLVfGazm"
   },
   "outputs": [],
   "source": [
    "tflite_pruning_model_file = '8_digits_mnist_model_pruning.tflite'\n",
    "interpreter_pruning = tf.lite.Interpreter(model_path=str(tflite_pruning_model_file))\n",
    "interpreter_pruning.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xbFKW__oGlGr",
    "outputId": "abe6db9c-4c3f-430f-98eb-6e38e7d84128"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'time': 0.9892525672912598, 'accuracy': 0.7401},\n",
       " {'Time to only Infer': 9.942054748535156e-05,\n",
       "  'Time to load Quantized model and Infer ': 0.9893736839294434})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(interpreter_pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y7a2UdyWGGW-"
   },
   "source": [
    "### Convert Pruned CNN (3_digits_mnist_model_pruning.h5) to Int with float fallback quantized version (9_digits_mnist_model_pruning_int_with_float.tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tX1LuS_sGHLa",
    "outputId": "cd3e5b23-ba1c-4106-84d1-543303f199ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved quantized and pruned TFLite model to: 9_digits_mnist_model_pruning_int_with_float.tflite\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_and_pruned_tflite_model = converter.convert()\n",
    "\n",
    "quantized_and_pruned_tflite_file = tflite_models_dir/'9_digits_mnist_model_pruning_int_with_float.tflite'\n",
    "\n",
    "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
    "  f.write(quantized_and_pruned_tflite_model)\n",
    "\n",
    "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZySi1TATGOQB"
   },
   "outputs": [],
   "source": [
    "tflite_pruning_10X_model_file = '9_digits_mnist_model_pruning_int_with_float.tflite'\n",
    "interpreter_pruning_10X = tf.lite.Interpreter(model_path=str(tflite_pruning_10X_model_file))\n",
    "interpreter_pruning_10X.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tSslV2q-Hcc1",
    "outputId": "5a238ffe-2bb6-4f71-c49b-f87d9e6d3204"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'time': 1.088677167892456, 'accuracy': 0.7396},\n",
       " {'Time to only Infer': 0.00011301040649414062,\n",
       "  'Time to load Quantized model and Infer ': 1.0888197422027588})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(interpreter_pruning_10X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8HsO71gEIDqy"
   },
   "source": [
    "### Convert Pruned CNN (3_digits_mnist_model_pruning.h5) to Float 16 Quantized version (10_digits_mnist_float16_purning_model.tflite) ###  Purning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42972"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_fp16_purning_model = converter.convert()\n",
    "tflite_model_fp16_purning_file = tflite_models_dir/\"10_digits_mnist_float16_purning_model.tflite\"\n",
    "tflite_model_fp16_purning_file.write_bytes(tflite_fp16_purning_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'time': 1.0293407440185547, 'accuracy': 0.7402},\n",
       " {'Time to only Infer': 0.00011658668518066406,\n",
       "  'Time to load Quantized model and Infer ': 1.029484748840332})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_float16_purning_model_file = tflite_models_dir/'10_digits_mnist_float16_purning_model.tflite'\n",
    "interpreter_pruning_float16_purning_model = tf.lite.Interpreter(model_path=str(tflite_float16_purning_model_file))\n",
    "interpreter_pruning_float16_purning_model.allocate_tensors()\n",
    "evaluate_model(interpreter_pruning_float16_purning_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8HsO71gEIDqy"
   },
   "source": [
    "### Convert Pruned CNN (3_digits_mnist_model_pruning.h5) to Int only Quantized version (11_digits_mnist_Integer_purning_model.tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, _ = tf.keras.datasets.mnist.load_data()\n",
    "images = tf.cast(mnist_train[0], tf.float32) / 255.0\n",
    "mnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\n",
    "def representative_data_gen():\n",
    "  for input_value in mnist_ds.take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23208"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "tflite_int_purning_quant_model = converter.convert()\n",
    "tflite_model_integeronly_purning_file = tflite_models_dir/\"11_digits_mnist_Integer_purning_model.tflite\"\n",
    "tflite_model_integeronly_purning_file.write_bytes(tflite_int_purning_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'time': 1.5820226669311523, 'accuracy': 0.947},\n",
       " {'Time to only Infer': 0.00015592575073242188,\n",
       "  'Time to load Quantized model and Infer ': 1.5821983814239502})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_intonly_purning_model_file = tflite_models_dir/'11_digits_mnist_Integer_purning_model.tflite'\n",
    "interpreter_intonly_purning_model = tf.lite.Interpreter(model_path=str(tflite_intonly_purning_model_file))\n",
    "interpreter_intonly_purning_model.allocate_tensors()\n",
    "evaluate_model(interpreter_intonly_purning_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXp7OIG2I948"
   },
   "source": [
    "### Create TFlite version (12_digits_mnist_model_qaware.tflite) of qaware trained CNN2 (2_digits_mnist_model_qaware.h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xi4QH5V9I4-8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved qaware trained TFLite model to: 12_digits_mnist_model_qaware.tflite\n"
     ]
    }
   ],
   "source": [
    "# model = tf.keras.models.load_model('./digits_mnist_model_qaware.h5', custom_objects=None, compile=True)\n",
    "# Cannot load a saved qaware .h5\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "quantized_tflite_file = tflite_models_dir/'12_digits_mnist_model_qaware.tflite'\n",
    "\n",
    "with open(quantized_tflite_file, 'wb') as f:\n",
    "  f.write(quantized_tflite_model)\n",
    "\n",
    "print('Saved qaware trained TFLite model to:', quantized_tflite_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sbIvb0ydJafz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1_RpvB7uJgQE",
    "outputId": "9d8cd9b7-1c3c-4489-e237-8bd90883982e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'time': 1.4810619354248047, 'accuracy': 0.1587},\n",
       " {'Time to only Infer': 0.0002460479736328125,\n",
       "  'Time to load Quantized model and Infer ': 1.4814565181732178})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(interpreter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXp7OIG2I948"
   },
   "source": [
    "### Convert qaware trained CNN (2_digits_mnist_model_qaware.h5) to Int with float quantized version (13_digits_mnist_Int_float_qaware_model.tflite) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "mnist_train, _ = tf.keras.datasets.mnist.load_data()\n",
    "images = tf.cast(mnist_train[0], tf.float32) / 255.0\n",
    "mnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\n",
    "def representative_data_gen():\n",
    "  for input_value in mnist_ds.take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24064"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_qaware_int_float = converter.convert()\n",
    "tflite_model_quant_file = tflite_models_dir/\"13_digits_mnist_Int_float_qaware_model.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_model_qaware_int_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'time': 2.072984457015991, 'accuracy': 0.9275},\n",
       " {'Time to only Infer': 0.00019884109497070312,\n",
       "  'Time to load Quantized model and Infer ': 2.0732014179229736})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_qaware_model_file = tflite_models_dir/'13_digits_mnist_Int_float_qaware_model.tflite'\n",
    "interpreter_tflite_qaware = tf.lite.Interpreter(model_path=str(tflite_qaware_model_file))\n",
    "interpreter_tflite_qaware.allocate_tensors()\n",
    "evaluate_model(interpreter_tflite_qaware)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXp7OIG2I948"
   },
   "source": [
    "### Convert qaware trained CNN (2_digits_mnist_model_qaware.h5) to Float16 quantized version (14_digits_mnist_float16_qaware.tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43568"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_fp16_model = converter.convert()\n",
    "tflite_model_fp16_file = tflite_models_dir/\"14_digits_mnist_float16_qaware.tflite\"\n",
    "tflite_model_fp16_file.write_bytes(tflite_fp16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'time': 1.4069643020629883, 'accuracy': 0.1584},\n",
       " {'Time to only Infer': 0.00014066696166992188,\n",
       "  'Time to load Quantized model and Infer ': 1.4071266651153564})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_qaware_float16_model_file = tflite_models_dir/'14_digits_mnist_float16_qaware.tflite'\n",
    "interpreter_tflite_qaware_f16 = tf.lite.Interpreter(model_path=str(tflite_qaware_float16_model_file))\n",
    "interpreter_tflite_qaware_f16.allocate_tensors()\n",
    "evaluate_model(interpreter_tflite_qaware_f16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXp7OIG2I948"
   },
   "source": [
    "### Int only quantization of Quantization-aware trained CNN2 - NA Quantization not yet supported for op: FAKE_QUANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Quantization not yet supported for op: FAKE_QUANT",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-42514a765c23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_input_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m  \u001b[0;31m# or tf.uint8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference_output_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m  \u001b[0;31m# or tf.uint8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtflite_int_quant_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mtflite_model_integeronly_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtflite_models_dir\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m\"digits_mnist_int_qaware.tflite\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtflite_model_integeronly_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtflite_int_quant_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/LR/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    467\u001b[0m       result = self._calibrate_quantize_model(\n\u001b[1;32m    468\u001b[0m           \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLOAT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLOAT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           self.experimental_new_quantizer)\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/LR/lib/python3.6/site-packages/tensorflow_core/lite/python/lite.py\u001b[0m in \u001b[0;36m_calibrate_quantize_model\u001b[0;34m(self, result, inference_input_type, inference_output_type, enable_mlir_quantizer)\u001b[0m\n\u001b[1;32m    241\u001b[0m     return calibrate_quantize.calibrate_and_quantize(\n\u001b[1;32m    242\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentative_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minference_input_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         inference_output_type, allow_float, enable_mlir_quantizer)\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_base_converter_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/LR/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py\u001b[0m in \u001b[0;36mcalibrate_and_quantize\u001b[0;34m(self, dataset_gen, input_type, output_type, allow_float, enable_mlir_quantizer)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         enable_mlir_quantizer)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m   def calibrate_and_quantize_single(self, dataset_gen, input_type, output_type,\n",
      "\u001b[0;32m~/.virtualenvs/LR/lib/python3.6/site-packages/tensorflow_core/lite/python/optimize/tensorflow_lite_wrap_calibration_wrapper.py\u001b[0m in \u001b[0;36mQuantizeModel\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mQuantizeModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensorflow_lite_wrap_calibration_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalibrationWrapper_QuantizeModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0mCalibrationWrapper_swigregister\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensorflow_lite_wrap_calibration_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalibrationWrapper_swigregister\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0mCalibrationWrapper_swigregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCalibrationWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Quantization not yet supported for op: FAKE_QUANT"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "tflite_model = converter.convert()\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "mnist_train, _ = tf.keras.datasets.mnist.load_data()\n",
    "images = tf.cast(mnist_train[0], tf.float32) / 255.0\n",
    "mnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\n",
    "def representative_data_gen():\n",
    "  for input_value in mnist_ds.take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "tflite_int_quant_model = converter.convert()\n",
    "tflite_model_integeronly_file = tflite_models_dir/\"digits_mnist_int_qaware.tflite\"\n",
    "tflite_model_integeronly_file.write_bytes(tflite_int_quant_model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Fashion_mnist_V4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
